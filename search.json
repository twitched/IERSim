[
  {
    "objectID": "Avatars.html",
    "href": "Avatars.html",
    "title": "IER Avatars",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\nTo create simulated insufficient error response data, we will use avatars. Each avatar has a name and an IER pattern.\nWe will also need a survey structure the avatars will fill. We can obtain the structure from the model. However, the structure is not the only part needed. We will also need to set the order of the survey since some avatars have a pattern that depends on the order of the survey. We will also need to add marker variables for CMV.\nFor now we will use the order of the data as the order of the survey and add in a CMV marker variable. We will assume that all of our simulated data is from users who always choose “Strongly Agree” for the CMV marker variable.\nCode\n#get the simulated data\nsim_data &lt;- readr::read_csv(file.path(getwd(), \"_data\", \"baseline.csv\"))\n\n\nRows: 500 Columns: 30\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (30): ss1, ss2, ss3, dj1, dj2, dj3, pj1, pj2, pj3, pj4, pj5, pj6, pj7, i...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nsim_data",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-1-mallory",
    "href": "Avatars.html#avatar-1-mallory",
    "title": "IER Avatars",
    "section": "Avatar 1: Mallory",
    "text": "Avatar 1: Mallory\nMallory is an accountant at a hotel management company. The HR manager at Mallory’s company is friends with a professor at a local university, so about once every year, Mallory and her coworkers are asked to complete surveys for research studies. As an incentive for participating in a survey when asked, if at least 50% of the company completes the survey, the office closes early the Friday after the survey closes. Most of the surveys ask about work environment and behaviors, including citizenship behaviors, organizational justice, workplace deviance, and job performance. When Mallory completed the first few surveys, she paid close attention to the survey items and answered thoughtfully and honestly, thinking the results of the survey might lead to improvements in the office (and a half-day off work). However, after about five years of taking these surveys, she realized changes weren’t coming, only the half-day off. At that point, her only motivation for participating in the survey was to make sure she did her part to get the survey participation rate up to the goal that was set for the survey. As a result, Mallory has stopped reading the questions in the survey altogether and just selects “neutral” for every item.\n\nGenerate Data\n\n\nCode\n# this function returns one mallory row with the structure of example_data\nmallory &lt;- function(example_data) {example_data[1,] |&gt; mutate(across(everything(), ~4))}\n\nmallory(sim_data)\n\n\n\n  \n\n\n\nWe only need one line of Mallory. To get multiple lines for insertion into a dataset, we can use replicate:\n\n\nCode\n#For example, to get 5 lines of Mallories\nmallories &lt;- function(num_rows, example_data) {replicate(num_rows, mallory(example_data), simplify = FALSE) |&gt; bind_rows()}\nmallories(5, sim_data)\n\n\n\n  \n\n\n\n\n\nGraph\n\n\nCode\n# Note: the histograms function is defined in the setup block of this page\nmallories(1, sim_data) |&gt; histograms()",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-2-darren",
    "href": "Avatars.html#avatar-2-darren",
    "title": "IER Avatars",
    "section": "Avatar 2: Darren",
    "text": "Avatar 2: Darren\nDarren is an attorney and works for the same company as Mallory. He’s never been a fan of the surveys because frankly, he feels like he is way too busy for something as silly as academic research. However, everyone hounds him to complete the surveys when they come around because of the promise of a half-day off work if the participation rate for the survey is high enough. He’s worried that if he doesn’t appear to take the survey seriously, his responses won’t be recorded, rendering his time completing the survey completely wasted. So, to appear like he’s thoughtfully responding to the survey, he randomly selects responses to the survey items so that there’s no risk of his responses being scrapped.\n\nGenerate Data\n\n\nCode\n# Darren's pattern is to randomly select a response for each item.  For Darren, we need to know how many we need in advance\n\n# This function will return num_rows darrens with the structure of example_data\ndarren &lt;- function(num_rows, example_data) {\n  1:num_rows |&gt; \n    map(\\(x) example_data[1,] |&gt; mutate(across(everything(), ~sample(1:7, 1)))) |&gt; \n    list_rbind()\n}\n\ndarren(10, sim_data)\n\n\n\n  \n\n\n\n\n\nGraph\n\n\nCode\ndarren(500, sim_data) |&gt; histograms()",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-3-bart",
    "href": "Avatars.html#avatar-3-bart",
    "title": "IER Avatars",
    "section": "Avatar 3: Bart",
    "text": "Avatar 3: Bart\nBart is a financial analyst who lives and works in a large metropolitan area. He commutes to work by train every day, and the trip is about 45 minutes each way. His work is stressful and draining, so sometimes he completes surveys online to wind down after a long day. He gets paid to do it, but that isn’t even something he thinks about when selecting surveys to complete. Bart’s “payment” is the enjoyment he gets from completing surveys. He likes to be introspective, likening it to meditation, so completing surveys is fascinating to him. So, when Bart completes a survey, he gives it his full attention. Well, he gives it his full attention for as long as he can. Some surveys are just too long, and he gets fatigued by repetitive questions that seem to go on and on. When that happens, Bart tries to give his brain a break by skimming questions or randomly allowing himself to zone out while reading questions after he’s been in a survey for a while.\n\nComments\nThis is the trickier one. Currently, our survey is too short to simulate Bart. Christie and I had talked about needing to add more variables to our simulation for this. However, Christie mentioned that they have always kept the scales short for our CMV studies because CMV is more likely to be detected that way. Are we going to be washing out the CMV possibility with a longer set of items? My question is—how does a longer list of short scales affect our ability to detect CMV? I’d assume in a similar way, right?\n\n\nGenerate Data\n\n\nCode\n# change the given data to look like a set of Bart responses\nbart &lt;- function(existing_data) {\n  # randomly select about half of the columns to be Bart's random point of zoning out\n  col_num &lt;- length(names(existing_data)) \n  existing_data |&gt; \n    # use row numbers to pivot each respondent into a group\n    mutate (row_num = row_number()) |&gt;\n    pivot_longer(cols = -row_num, names_to = \"question\", values_to = \"response\") |&gt; \n    group_by(row_num) |&gt;\n    # randomly select about half of the columns to be Bart's random point of zoning out\n    group_modify ( ~{\n      about_half &lt;- rnorm(1, col_num/2, col_num/10) |&gt; round()\n      \n      # keep the first half of the columns\n      start &lt;- .x |&gt; slice_head(n = col_num - about_half)\n      \n      # replace the second half of the columns with random responses\n      end &lt;- .x |&gt; slice_tail(n = about_half) |&gt;\n      mutate(response = sample(1:7, about_half, replace = TRUE))\n      \n      # put them back together\n      rbind(start, end) \n    }) |&gt;\n    # put the data back into tabular format\n    pivot_wider(names_from = question, values_from = response) |&gt; ungroup() |&gt; select(-row_num)\n }\n\nbart(slice_sample(sim_data, n = 10))\n\n\n\n  \n\n\n\n\n\nGraph\n\n\nCode\nbart(sim_data) |&gt; histograms()",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "Avatars.html#avatar-4-scarlett",
    "href": "Avatars.html#avatar-4-scarlett",
    "title": "IER Avatars",
    "section": "Avatar 4: Scarlett",
    "text": "Avatar 4: Scarlett\nScarlett is an office manager who spends most of her day at work filing paperwork and ordering office supplies. She has quite a bit of downtime at work, so she completes surveys to earn some extra cash. However, she has often had to rush through completing surveys due to being called into meetings last minute. Scarlett has read on blogs and message boards that researchers often use trap questions and consistency screening methods to catch participants who are rushing or careless, and those participants don’t get paid for their work. So, when she must rush through a survey (whether it’s halfway through a survey or for its entirety), she’s come up with her own system. First, she reads the first question on the page to decide how to answer the rest of the items. Since she doesn’t want to seem too consistent, she varies her answers for the rest of the page around her answer to the first item. Then, while answering the items on the rest of the page in a nonrandom pattern, she pays just enough attention to the survey items to spot catch questions such as, “Please mark STRONGLY AGREE for this item,” or “I am paid biweekly by gremlins,” so that she will answer those items correctly.\n\nComments\nFor this avatar, we’d need the response to the first item on a page to be selected randomly. Then, each subsequent response is a random choice within a constrained range around the first response. For example, if “4” is the response for the first item, subsequent items on the page should be a random selection of 3, 4, or 5.\n\n\nGenerate Data\nTo make this work, we need to indicate which items are on a page.\n\n\nCode\n# We'll create a tibble with each variable and on what page it appears\npage_table &lt;- tribble(\n  ~question, ~page,\n   \"ssr1\",   1,\n   \"ssr2\",   1,\n   \"ssr3\",   1,\n   \"ri1\",    1,\n   \"ri2\",    1,\n   \"ri3\",    1,\n   \"wom1\",   1,\n   \"wom2\",   1,\n   \"wom3\",   1,\n   \"cmv1\",   1,\n   \"sat1\",   2,\n   \"sat2\",   2,\n   \"sat3\",   2,\n   \"dj1\",    2,\n   \"dj2\",    2,\n   \"dj3\",    2,\n   \"dj4\",    2,\n   \"ij1\",    3,\n   \"ij2\",    3,\n   \"ij3\",    3,\n   \"ij4\",    3,\n   \"pj1\",    3,\n   \"pj2\",    3,\n   \"pj3\",    3,\n   \"pj4\",    3\n)\n\n# This function will return num_rows scarletts with the structure of example_data and the page structure abov\nscarlett &lt;- function(num_rows, page_table) {\n  1:num_rows |&gt; \n    map(\\(x) {\n      page_table |&gt; group_by(page) |&gt; \n        group_modify(~{\n          # give every question on the page the same value\n          g &lt;- .x |&gt; mutate(value = sample(1:7, 1))\n          # create a vector of normally distributed adjustments for the all questions \n          # with the first being zero\n          adjustments &lt;- c(0, rnorm(nrow(g) - 1, 0, 1) |&gt; round())\n          # adjust each value by the random amount\n          g |&gt; mutate(value = value + adjustments) |&gt;\n            # make sure the values are between 1 and 7\n            mutate(value = pmax(1, pmin(7, value)))\n        }) |&gt; ungroup() |&gt; select(-page) |&gt;\n        # put into tabular format\n        pivot_wider(names_from = question, values_from = value) \n    }) |&gt;\n    list_rbind()\n}\n\nscarlett(10, page_table)\n\n\n\n  \n\n\n\n\n\nGraph\n\n\nCode\nscarlett(500, page_table) |&gt; histograms()",
    "crumbs": [
      "IER Avatars"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CMV-IER Simulation Main",
    "section": "",
    "text": "This is a set of pages that run through simulating and testing data for the CMV IER project by Elizabeth Raglund, Christie Fuller, Marcia Simmering, and Doug Twitchell.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#ier-simulation",
    "href": "index.html#ier-simulation",
    "title": "CMV-IER Simulation Main",
    "section": "",
    "text": "This is a set of pages that run through simulating and testing data for the CMV IER project by Elizabeth Raglund, Christie Fuller, Marcia Simmering, and Doug Twitchell.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#generate-baseline-data",
    "href": "index.html#generate-baseline-data",
    "title": "CMV-IER Simulation Main",
    "section": "Generate Baseline Data",
    "text": "Generate Baseline Data\nIn this simulation, we generate a data set with no IER. This is the baseline data set that we will use to compare to the other data sets.\nGenerate Baseline Data",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#generate-data-from-ier-avatars",
    "href": "index.html#generate-data-from-ier-avatars",
    "title": "CMV-IER Simulation Main",
    "section": "Generate Data from IER Avatars",
    "text": "Generate Data from IER Avatars\nGenerate Data from IER Avatars",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#analyze-each-avatar",
    "href": "index.html#analyze-each-avatar",
    "title": "CMV-IER Simulation Main",
    "section": "Analyze each Avatar",
    "text": "Analyze each Avatar\nTBD",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html",
    "href": "GenerateBaselineData.html",
    "title": "GenerateBaselineData",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#introduction",
    "href": "GenerateBaselineData.html#introduction",
    "title": "GenerateBaselineData",
    "section": "Introduction",
    "text": "Introduction\nIn this document we generate a random dataset using the model analyzed in (Kurtessis et al. 2017)",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#the-model",
    "href": "GenerateBaselineData.html#the-model",
    "title": "GenerateBaselineData",
    "section": "The model",
    "text": "The model\nThe model we will use has the constructs in Table 1 below\n\n\n\n\n\n\n\n\nConstruct\nAbbreviation\nNumber of measures\n\n\n\n\nSupervisor Supporta\nSS\n3\n\n\nFairnessb\n\n\n\n\n Distributed Justice\nDJ\n3\n\n\n Procedural Justice\nPJ\n7\n\n\n Interpersonal Justice\nIJ\n7\n\n\n Information Justice\nINFOJ\n4\n\n\nPositive Affectivityc\nPA\n3\n\n\nNegative Affectivityd\nNEGA\n4\n\n\nPerceived Organizational Supporte\nPOS\n3\n\n\n\n\na(Eisenberger et al. 2002)\nbComposite of Distributed, Procedural, Interpersonal, and Information Justice as defined in (Skarlicki, van Jaarsveld, and Walker 2008) and (Kurtessis et al. 2017)\nc(Tellegen, Watson, and Clark 1999)\nd(Tellegen, Watson, and Clark 1999)\neThree items highest items from (Eisenberger et al. 1986) as used in (Eisenberger et al. 2002)\n\nEach of these constructs has several measures, and the model is expressed as a confirmatory factor analysis in the lavaan equation below and associated diagram\n\n\nCode\ncfa_sim_model &lt;- \"\n  # Measurement model\n  SS =~ ss1 + ss2 + ss3\n  \n  #Fairness is made up of DJ, PJ, IJ, and INFOJ\n  DJ  =~ dj1 + dj2 + dj3\n  PJ  =~ pj1 + pj2 + pj3 + pj4 + pj5 + pj6 + pj7\n  IJ  =~ ij1 + ij2 + ij3 + ij4\n  INFOJ  =~ infoj1 + infoj2 + infoj3 + infoj4\n  \n  PA =~ pa1 + pa2 + pa3\n  NEGA =~ nega1 + nega2 + nega3\n  POS =~ pos1 + pos2 + pos3\n\n  #Structural Model of Fairness\n  F =~ DJ + PJ + IJ + INFOJ\n\n  # Variances\n  SS ~~ SS\n  F ~~ F\n  DJ ~~ DJ\n  PJ ~~ PJ\n  IJ ~~ IJ\n  INFOJ ~~ INFOJ\n  NEGA ~~ NEGA\n  PA ~~ PA\n  POS ~~ POS\n\"",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#the-data",
    "href": "GenerateBaselineData.html#the-data",
    "title": "GenerateBaselineData",
    "section": "The Data",
    "text": "The Data\nThe data we have to inform the simulation include the correlations among the latent variables and the factor loadings\n\nLatent Variable Correlations\n\n\n\nSS\nF\nPA\nNegA\nPOS\n\n\n\n\nSS\n—\n\n\n\n\n\n\nF\n.61\n—\n\n\n\n\n\nPA\n.33\n.26\n—\n\n\n\n\nNegA\n–.30\n–.22\n–.36\n—\n\n\n\nPOS\n.60\n.70\n.34\n–.43\n—\n\n\n\n\nFactor Loadings\n\n\nLatent Variables\nIndicators\nFactor Loadings\n\n\n\n\nS\nss1\n.71\n\n\n\nss2\n.74\n\n\n\nss3\n.83\n\n\nDJ\ndj1\n.90\n\n\n\ndj2\n.96\n\n\n\ndj3\n.95\n\n\nPJ\npj1\n.70\n\n\n\npj2\n.65\n\n\n\npj3\n.72\n\n\n\npj4\n.76\n\n\n\npj5\n.82\n\n\n\npj6\n.72\n\n\n\npj7\n.61\n\n\nIJ\nij1\n.89\n\n\n\nij2\n.96\n\n\n\nij3\n.96\n\n\n\nij4\n.78\n\n\nINFOJ\ninfoj1\n.77\n\n\n\ninfoj2\n.92\n\n\n\ninfoj3\n.93\n\n\n\ninfoj4\n.85\n\n\nPA\npa1\n.81\n\n\n\npa2\n.80\n\n\n\npa3\n.76\n\n\nNEGA\nnega1\n.89\n\n\n\nnega2\n.88\n\n\n\nnega3\n.87\n\n\nPOS\npos1\n.72\n\n\n\npos2\n.76\n\n\n\npos3\n.80",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#simulating-new-data",
    "href": "GenerateBaselineData.html#simulating-new-data",
    "title": "GenerateBaselineData",
    "section": "Simulating New Data",
    "text": "Simulating New Data\n\n\nCode\n# Install and load the lavaan package if you haven't already\n# install.packages(\"lavaan\")\nlibrary(lavaan)\n\n# Define the lavaan model syntax\nmodel &lt;- '\n  # Measurement model\n  SS =~ ss1 + ss2 + ss3\n\n  #Fairness is made up of DJ, PJ, IJ, and INFOJ\n  DJ  =~ dj1 + dj2 + dj3\n  PJ  =~ pj1 + pj2 + pj3 + pj4 + pj5 + pj6 + pj7\n  IJ  =~ ij1 + ij2 + ij3 + ij4\n  INFOJ  =~ infoj1 + infoj2 + infoj3 + infoj4\n\n  PA =~ pa1 + pa2 + pa3\n  NEGA =~ nega1 + nega2 + nega3\n  POS =~ pos1 + pos2 + pos3\n\n  #F is a higher order factor\n  F =~ DJ + PJ + IJ + INFOJ\n\n  # Latent variable correlations (specified as covariances in lavaan for simulation)\n  # We need to translate correlations into covariances. If latent variables\n  # are standardized to have variance 1, then correlations equal covariances.\n  # Assuming latent variables have variance 1 for ease of simulation \n\n  SS ~~ 1*SS # Fix variance of latent variables to 1 for correlation interpretation\n  F ~~ 1*F\n  DJ ~~ 1*DJ\n  PJ ~~ 1*PJ\n  IJ ~~ 1*IJ\n  INFOJ ~~ 1*INFOJ\n  PA ~~ 1*PA\n  NEGA ~~ 1*NEGA\n  POS ~~ 1*POS\n\n  SS ~~ 0.61*F\n  SS ~~ 0.33*PA\n  SS ~~ -0.30*NEGA\n  SS ~~ 0.60*POS\n  F ~~ 0.26*PA\n  F ~~ -0.22*NEGA\n  F ~~ 0.70*POS\n  PA ~~ -0.36*NEGA\n  PA ~~ 0.34*POS\n  NEGA ~~ -0.43*POS\n\n  # Specify factor loadings\n  SS =~ 0.71*ss1 + 0.74*ss2 + 0.83*ss3\n  DJ =~ 0.90*dj1 + 0.96*dj2 + 0.95*dj3\n  PJ =~ 0.70*pj1 + 0.65*pj2 + 0.72*pj3 + 0.76*pj4 + 0.82*pj5 + 0.72*pj6 + 0.61*pj7\n  IJ =~ 0.89*ij1 + 0.96*ij2 + 0.96*ij3 + 0.78*ij4\n  INFOJ =~ 0.77*infoj1 + 0.92*infoj2 + 0.93*infoj3 + 0.85*infoj4\n  PA =~ 0.81*pa1 + 0.80*pa2 + 0.76*pa3\n  NEGA =~ 0.89*nega1 + 0.88*nega2 + 0.87*nega3\n  POS =~ 0.72*pos1 + 0.76*pos2 + 0.80*pos3\n  # Factor loadings for F from DJ, PJ, IJ, INFOJ - assuming these are treated as indicators of F\n  F =~ 0.65*DJ + 0.65*PJ + 0.65*IJ + 0.65*INFOJ\n\n  # Variances of indicators (error variances) are implicitly estimated by simulateData()\n  # based on the specified factor loadings and latent variances.\n'\n\n# Set the number of observations\nn_obs &lt;- 500 # You can change this to your desired sample size\n\n# Simulate the data\nset.seed(123) # for reproducibility\ncfa_data &lt;- simulateData(model, sample.nobs = n_obs)\n\n# Display the first few rows of the simulated data\nhead(cfa_data)\n\n\n\n  \n\n\n\nCode\n# You can now use the 'simulated_data' data frame for analysis or other purposes.\n\n\n#Transform to likert scale\n\n\nCode\n# --- Transform continuous data to a 1-7 Likert scale ---\n\n# Get the names of the indicator variables\nindicator_names &lt;- c(\n  \"ss1\", \"ss2\", \"ss3\",\n  \"dj1\", \"dj2\", \"dj3\",\n  \"pj1\", \"pj2\", \"pj3\", \"pj4\", \"pj5\", \"pj6\", \"pj7\",\n  \"ij1\", \"ij2\", \"ij3\", \"ij4\",\n  \"infoj1\", \"infoj2\", \"infoj3\", \"infoj4\",\n  \"pa1\", \"pa2\", \"pa3\",\n  \"nega1\", \"nega2\", \"nega3\",\n  \"pos1\", \"pos2\", \"pos3\"\n)\n\n# Create a new data frame for Likert data\nlikert_data &lt;- cfa_data\n\n# Function to transform a continuous vector to 1-7 scale\ntransform_to_likert &lt;- function(x) {\n  # Scale the data to a reasonable range for Likert (e.g., -3.5 to 3.5)\n  # and shift/scale to 1-7. This is a heuristic approach.\n  # A more sophisticated approach would involve thresholds based on a desired\n  # distribution or using packages designed for simulating ordinal data.\n\n  # Simple linear scaling to approx 0-8 range then round and clamp to 1-7\n  # This assumes the continuous data has a mean around 0.\n  min_val &lt;- min(x, na.rm = TRUE)\n  max_val &lt;- max(x, na.rm = TRUE)\n\n  # Scale to a range slightly larger than 1-7 (e.g., 0.5 to 7.5)\n  scaled_x &lt;- 0.5 + (x - min_val) / (max_val - min_val) * 7\n\n  # Round to the nearest integer\n  rounded_x &lt;- round(scaled_x)\n\n  # Clamp values to be within 1 and 7\n  likert_x &lt;- pmax(1, pmin(7, rounded_x))\n\n  return(likert_x)\n}\n\n# Apply the transformation to each indicator column\nfor (col_name in indicator_names) {\n  if (col_name %in% colnames(likert_data)) {\n    likert_data[[col_name]] &lt;- transform_to_likert(likert_data[[col_name]])\n  }\n}\n\n# Display the first few rows of the simulated Likert data\nhead(likert_data)\n\n\n\n  \n\n\n\nCode\n# Get summary statistics to check the distribution\nsummary(likert_data)\n\n\n      ss1             ss2             ss3             dj1             dj2      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00  \n 1st Qu.:3.000   1st Qu.:4.000   1st Qu.:3.000   1st Qu.:3.000   1st Qu.:3.00  \n Median :3.000   Median :5.000   Median :4.000   Median :4.000   Median :4.00  \n Mean   :3.312   Mean   :4.494   Mean   :3.808   Mean   :3.898   Mean   :4.12  \n 3rd Qu.:4.000   3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:5.00  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.00  \n      dj3             pj1             pj2             pj3       \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.:3.000   1st Qu.:3.000   1st Qu.:3.000  \n Median :3.000   Median :4.000   Median :4.000   Median :4.000  \n Mean   :3.438   Mean   :4.006   Mean   :4.122   Mean   :4.284  \n 3rd Qu.:4.000   3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n      pj4             pj5             pj6            pj7             ij1       \n Min.   :1.000   Min.   :1.000   Min.   :1.00   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000   1st Qu.:4.000   1st Qu.:3.00   1st Qu.:3.000   1st Qu.:3.000  \n Median :4.000   Median :5.000   Median :4.00   Median :4.000   Median :4.000  \n Mean   :4.326   Mean   :4.564   Mean   :4.03   Mean   :4.004   Mean   :3.952  \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:5.00   3rd Qu.:5.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.00   Max.   :7.000   Max.   :7.000  \n      ij2             ij3             ij4            infoj1         infoj2     \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.:3.000   1st Qu.:3.000   1st Qu.:3.00   1st Qu.:3.000  \n Median :4.000   Median :4.000   Median :4.000   Median :4.00   Median :4.000  \n Mean   :4.138   Mean   :3.674   Mean   :3.744   Mean   :4.19   Mean   :4.094  \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4.000   3rd Qu.:5.00   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.00   Max.   :7.000  \n     infoj3         infoj4           pa1             pa2             pa3       \n Min.   :1.00   Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.00   1st Qu.:3.000   1st Qu.:3.000   1st Qu.:3.000   1st Qu.:3.000  \n Median :4.00   Median :4.000   Median :3.000   Median :4.000   Median :4.000  \n Mean   :4.48   Mean   :3.828   Mean   :3.464   Mean   :4.096   Mean   :3.792  \n 3rd Qu.:5.00   3rd Qu.:5.000   3rd Qu.:4.000   3rd Qu.:5.000   3rd Qu.:5.000  \n Max.   :7.00   Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n     nega1           nega2           nega3            pos1      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000   1st Qu.:3.000   1st Qu.:3.000   1st Qu.:4.000  \n Median :4.000   Median :4.000   Median :3.000   Median :4.000  \n Mean   :4.228   Mean   :4.144   Mean   :3.502   Mean   :4.378  \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000  \n      pos2            pos3     \n Min.   :1.000   Min.   :1.00  \n 1st Qu.:3.000   1st Qu.:4.00  \n Median :4.000   Median :4.00  \n Mean   :4.018   Mean   :4.33  \n 3rd Qu.:5.000   3rd Qu.:5.00  \n Max.   :7.000   Max.   :7.00  \n\n\nPlot each variable to check if it is reasonable\n\n\nCode\nlikert_data |&gt; \n  pivot_longer(cols = everything()) |&gt; \n  ggplot(aes(value)) +\n  geom_histogram(binwidth = .5) +\n  scale_x_continuous(breaks = seq(1, 7, 1), limits = c(0.5, 7.5)) +\n  facet_wrap(vars(name))\n\n\n\n\n\n\n\n\n\nSave the simulated data to allow for later use.\n\n\nCode\nlikert_data |&gt; readr::write_csv(file.path(getwd(), \"_data\", \"baseline.csv\"))",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#download-the-data",
    "href": "GenerateBaselineData.html#download-the-data",
    "title": "GenerateBaselineData",
    "section": "Download the Data",
    "text": "Download the Data\nClick on the button to download the data\n\n\n Download data",
    "crumbs": [
      "Generate Baseline Data"
    ]
  },
  {
    "objectID": "GenerateBaselineData.html#check-the-simulated-data",
    "href": "GenerateBaselineData.html#check-the-simulated-data",
    "title": "GenerateBaselineData",
    "section": "Check the Simulated Data",
    "text": "Check the Simulated Data\nLet’s see what the results are when we try to fit the data to the model\n\n\nCode\nfit&lt;- cfa(cfa_sim_model, likert_data, meanstructure = TRUE)\nsummary(fit)\n\n\nlavaan 0.6-19 ended normally after 53 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                       104\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                               374.231\n  Degrees of freedom                               391\n  P-value (Chi-square)                           0.721\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  SS =~                                               \n    ss1               1.000                           \n    ss2               1.047    0.141    7.444    0.000\n    ss3               1.228    0.162    7.580    0.000\n  DJ =~                                               \n    dj1               1.000                           \n    dj2               0.994    0.085   11.647    0.000\n    dj3               0.942    0.080   11.698    0.000\n  PJ =~                                               \n    pj1               1.000                           \n    pj2               1.044    0.091   11.440    0.000\n    pj3               1.164    0.104   11.158    0.000\n    pj4               1.040    0.084   12.323    0.000\n    pj5               1.095    0.087   12.556    0.000\n    pj6               1.116    0.093   12.038    0.000\n    pj7               0.970    0.092   10.558    0.000\n  IJ =~                                               \n    ij1               1.000                           \n    ij2               0.987    0.075   13.193    0.000\n    ij3               1.113    0.082   13.574    0.000\n    ij4               0.718    0.065   10.997    0.000\n  INFOJ =~                                            \n    infoj1            1.000                           \n    infoj2            0.956    0.081   11.845    0.000\n    infoj3            1.079    0.090   11.962    0.000\n    infoj4            1.111    0.092   12.070    0.000\n  PA =~                                               \n    pa1               1.000                           \n    pa2               1.289    0.165    7.809    0.000\n    pa3               0.995    0.132    7.518    0.000\n  NEGA =~                                             \n    nega1             1.000                           \n    nega2             1.272    0.137    9.269    0.000\n    nega3             0.965    0.105    9.208    0.000\n  POS =~                                              \n    pos1              1.000                           \n    pos2              1.292    0.162    8.000    0.000\n    pos3              1.197    0.150    7.978    0.000\n  F =~                                                \n    DJ                1.000                           \n    PJ                0.861    0.142    6.051    0.000\n    IJ                0.960    0.166    5.797    0.000\n    INFOJ             1.022    0.166    6.153    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  SS ~~                                               \n    PA                0.140    0.033    4.179    0.000\n    NEGA             -0.116    0.029   -3.954    0.000\n    POS               0.202    0.035    5.712    0.000\n    F                 0.143    0.029    4.941    0.000\n  PA ~~                                               \n    NEGA             -0.164    0.036   -4.614    0.000\n    POS               0.170    0.035    4.784    0.000\n    F                 0.073    0.026    2.852    0.004\n  NEGA ~~                                             \n    POS              -0.122    0.030   -4.071    0.000\n    F                -0.009    0.022   -0.407    0.684\n  POS ~~                                              \n    F                 0.166    0.031    5.383    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ss1               3.312    0.049   67.871    0.000\n   .ss2               4.494    0.046   96.787    0.000\n   .ss3               3.808    0.051   74.477    0.000\n   .dj1               3.898    0.052   74.752    0.000\n   .dj2               4.120    0.051   80.016    0.000\n   .dj3               3.438    0.048   71.682    0.000\n   .pj1               4.006    0.049   81.705    0.000\n   .pj2               4.122    0.051   80.742    0.000\n   .pj3               4.284    0.059   72.886    0.000\n   .pj4               4.326    0.046   93.967    0.000\n   .pj5               4.564    0.047   96.696    0.000\n   .pj6               4.030    0.051   79.001    0.000\n   .pj7               4.004    0.052   76.326    0.000\n   .ij1               3.952    0.056   70.265    0.000\n   .ij2               4.138    0.053   78.568    0.000\n   .ij3               3.674    0.056   65.529    0.000\n   .ij4               3.744    0.048   77.648    0.000\n   .infoj1            4.190    0.051   82.751    0.000\n   .infoj2            4.094    0.051   80.378    0.000\n   .infoj3            4.480    0.057   79.058    0.000\n   .infoj4            3.828    0.058   66.468    0.000\n   .pa1               3.464    0.052   66.697    0.000\n   .pa2               4.096    0.059   69.141    0.000\n   .pa3               3.792    0.058   65.249    0.000\n   .nega1             4.228    0.046   90.971    0.000\n   .nega2             4.144    0.054   76.290    0.000\n   .nega3             3.502    0.045   78.543    0.000\n   .pos1              4.378    0.046   94.238    0.000\n   .pos2              4.018    0.059   68.001    0.000\n   .pos3              4.330    0.055   78.608    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    SS                0.323    0.066    4.910    0.000\n    F                 0.216    0.052    4.155    0.000\n   .DJ                0.446    0.069    6.450    0.000\n   .PJ                0.312    0.049    6.373    0.000\n   .IJ                0.558    0.078    7.137    0.000\n   .INFOJ             0.394    0.061    6.437    0.000\n    NEGA              0.428    0.069    6.224    0.000\n    PA                0.470    0.086    5.445    0.000\n    POS               0.345    0.063    5.442    0.000\n   .ss1               0.868    0.068   12.688    0.000\n   .ss2               0.724    0.062   11.658    0.000\n   .ss3               0.820    0.076   10.757    0.000\n   .dj1               0.697    0.065   10.784    0.000\n   .dj2               0.670    0.063   10.642    0.000\n   .dj3               0.563    0.055   10.282    0.000\n   .pj1               0.730    0.052   13.900    0.000\n   .pj2               0.788    0.057   13.880    0.000\n   .pj3               1.087    0.077   14.073    0.000\n   .pj4               0.548    0.042   13.048    0.000\n   .pj5               0.547    0.043   12.739    0.000\n   .pj6               0.712    0.053   13.365    0.000\n   .pj7               0.932    0.065   14.407    0.000\n   .ij1               0.825    0.068   12.203    0.000\n   .ij2               0.649    0.057   11.294    0.000\n   .ij3               0.634    0.064    9.975    0.000\n   .ij4               0.772    0.056   13.870    0.000\n   .infoj1            0.662    0.057   11.609    0.000\n   .infoj2            0.730    0.059   12.345    0.000\n   .infoj3            0.884    0.073   12.161    0.000\n   .infoj4            0.893    0.075   11.976    0.000\n   .pa1               0.879    0.080   11.031    0.000\n   .pa2               0.974    0.110    8.827    0.000\n   .pa3               1.224    0.098   12.517    0.000\n   .nega1             0.653    0.059   11.097    0.000\n   .nega2             0.783    0.083    9.468    0.000\n   .nega3             0.596    0.054   11.005    0.000\n   .pos1              0.734    0.061   12.111    0.000\n   .pos2              1.170    0.098   11.927    0.000\n   .pos3              1.023    0.085   12.002    0.000",
    "crumbs": [
      "Generate Baseline Data"
    ]
  }
]